# LLM provider selection: "openai" | "gemini" | "local" | "none"
#   local  → Ollama (tamamen offline, GPU/CPU ile calisan yerel LLM)
#   none   → Extractive mod (LLM olmadan dogrudan belgeden cevap)
LLM_PROVIDER=none

# OpenAI-compatible
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# Gemini
# Set LLM_PROVIDER=gemini to enable Gemini.
GEMINI_API_KEY=
# Model notes:
# - If you get 404 NOT_FOUND for a model name, it may not be enabled for your account/project.
# - Recommended fallback (widely available): gemini-2.0-flash
# - If available on your account: gemini-3-flash-preview
GEMINI_MODEL=gemini-3-flash-preview
# GEMINI_MODEL=gemini-2.0-flash

# Embeddings
# Embedding model selection:
# - auto: if CUDA is available, prefer multilingual-e5-base; otherwise multilingual-e5-small
# - or set a fixed model name (e.g. intfloat/multilingual-e5-small)
EMBEDDING_MODEL=auto
# Device selection for embeddings: auto | cpu | cuda
# - auto: use CUDA if available, else CPU
# - cpu: force CPU even if CUDA exists
# - cuda: prefer CUDA, fall back to CPU if unavailable
EMBEDDING_DEVICE=auto

# Storage
DATA_DIR=./data
CHROMA_DIR=./data/chroma

# OCR (optional)
# If tesseract isn't on PATH, set a full path, e.g.
# TESSERACT_CMD=C:\Program Files\Tesseract-OCR\tesseract.exe
TESSERACT_CMD=

# Tesseract language data folder (optional).
# Use this if you cannot write to Program Files (no admin) and want to keep traineddata files locally.
# Example:
# TESSDATA_PREFIX=C:\path\to\project\data\tessdata
TESSDATA_PREFIX=

# Advanced OCR (optional): pass through to pytesseract `config=`
# Examples:
# - "--psm 6 --oem 3"
# - "--psm 4"
TESSERACT_CONFIG=

# VLM (optional): multimodal extract-only text extraction for complex layouts (tables/multi-column).
# Values: off | auto | force
# Default behavior of the UI matches: force
VLM_MODE=force
# Safety cap: max pages per document to send to VLM
VLM_MAX_PAGES=50
# VLM provider: "gemini" (default) | "local" (Ollama vision model)
VLM_PROVIDER=gemini

# ── Local / Ollama (offline) ayarlari ────────────────────────────────────────
# Bu ayarlar yalnizca LLM_PROVIDER=local ve/veya VLM_PROVIDER=local iken kullanilir.
# Ollama kurulumu: https://ollama.com/download
#
# Ollama calistigin sunucu adresi:
OLLAMA_BASE_URL=http://localhost:11434
#
# Text LLM model adi (ollama pull <model> ile indir):
# Onerilen: qwen2.5:7b (6 GB VRAM icin Q4 quantize)
OLLAMA_LLM_MODEL=qwen2.5:7b
#
# Vision (multimodal) model adi:
# Onerilen: llava:7b (gorsel icerik cikarimi icin)
OLLAMA_VLM_MODEL=llava:7b
#
# HTTP istek zaman asimi (saniye):
OLLAMA_TIMEOUT=120

# Retrieval (optional): section subtree fetch depth for section_list intent.
# Default keeps current behavior (≈ children + grandchildren).
SECTION_FETCH_MAX_DEPTH=2

# Retrieval confidence (optional): the deterministic section-list rendering
# path now verifies that the query's topic words have lexical overlap with
# the matched section heading.  If there is no overlap the system falls
# through to the LLM path which can evaluate relevance and respond
# "Belgede bu bilgi bulunamadi." when appropriate.
# This setting requires no configuration — it is always active.

# Logging (optional): persist Q/A logs to JSONL (OFF by default).
# This logs the user query + answer + retrieval metadata.
RAG_LOG=0
RAG_LOG_DIR=./data/logs
# Also write a per-document log file under <dir>/by_doc/<doc>.jsonl
RAG_LOG_BY_DOC=1
# Include GenerationResult.context_preview (truncated) in logs.
RAG_LOG_CONTEXT_PREVIEW=0
# Clamp big strings to avoid huge logs.
RAG_LOG_MAX_TEXT_CHARS=4000

# Dev convenience (optional): if enabled, the server process will exit after the last
# browser tab disconnects. Helps avoid "port already in use" on restart.
AUTO_EXIT_ON_NO_CLIENTS=0
# Grace seconds before exiting (0-120).
AUTO_EXIT_GRACE_SECONDS=8

