# LLM provider selection: "openai" | "gemini" | "none"
LLM_PROVIDER=none

# OpenAI-compatible
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# Gemini
# Set LLM_PROVIDER=gemini to enable Gemini.
GEMINI_API_KEY=
# Model notes:
# - If you get 404 NOT_FOUND for a model name, it may not be enabled for your account/project.
# - Recommended fallback (widely available): gemini-2.0-flash
# - If available on your account: gemini-3-flash-preview
GEMINI_MODEL=gemini-3-flash-preview
# GEMINI_MODEL=gemini-2.0-flash

# Embeddings
EMBEDDING_MODEL=intfloat/multilingual-e5-small

# Storage
DATA_DIR=./data
CHROMA_DIR=./data/chroma

# OCR (optional)
# If tesseract isn't on PATH, set a full path, e.g.
# TESSERACT_CMD=C:\Program Files\Tesseract-OCR\tesseract.exe
TESSERACT_CMD=

# Tesseract language data folder (optional).
# Use this if you cannot write to Program Files (no admin) and want to keep traineddata files locally.
# Example:
# TESSDATA_PREFIX=C:\path\to\project\data\tessdata
TESSDATA_PREFIX=

# VLM (optional): multimodal extract-only text extraction for complex layouts (tables/multi-column).
# Values: off | auto | force
# Default behavior of the UI matches: force
VLM_MODE=force
# Safety cap: max pages per document to send to VLM
VLM_MAX_PAGES=25

# Retrieval (optional): section subtree fetch depth for section_list intent.
# Default keeps current behavior (â‰ˆ children + grandchildren).
SECTION_FETCH_MAX_DEPTH=2

# Dev convenience (optional): if enabled, the server process will exit after the last
# browser tab disconnects. Helps avoid "port already in use" on restart.
AUTO_EXIT_ON_NO_CLIENTS=0
# Grace seconds before exiting (0-120).
AUTO_EXIT_GRACE_SECONDS=8

